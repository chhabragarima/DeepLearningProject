{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "    num_classes = 21\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "\n",
    "    def cross_entropy_loss(self, x, y):\n",
    "        '''Cross entropy loss w/o averaging across all samples.\n",
    "        Args:\n",
    "          x: (tensor) sized [N,D].\n",
    "          y: (tensor) sized [N,].\n",
    "        Return:\n",
    "          (tensor) cross entroy loss, sized [N,].\n",
    "        '''\n",
    "        xmax = x.data.max()\n",
    "        print('x y size {} {}'.format(x.size(), y.size()))\n",
    "        log_sum_exp = torch.log(torch.sum(torch.exp(x-xmax), 1)) + xmax\n",
    "        print('log_sum_exp {}'.format(log_sum_exp.size()))\n",
    "        return log_sum_exp - x.gather(1, y.view(-1,1))\n",
    "\n",
    "    def hard_negative_mining(self, conf_loss, pos):\n",
    "        '''Return negative indices that is 3x the number as postive indices.\n",
    "        Args:\n",
    "          conf_loss: (tensor) cross entroy loss between conf_preds and conf_targets, sized [N*8732,].\n",
    "          pos: (tensor) positive(matched) box indices, sized [N,8732].\n",
    "        Return:\n",
    "          (tensor) negative indices, sized [N,8732].\n",
    "        '''\n",
    "        batch_size, num_boxes = pos.size()\n",
    "        conf_loss[pos.view(-1)] = 0  # set pos boxes = 0, the rest are neg conf_loss\n",
    "        conf_loss = conf_loss.view(batch_size, -1)  # [N,8732]\n",
    "\n",
    "        _,idx = conf_loss.sort(1, descending=True)  # sort by neg conf_loss\n",
    "        _,rank = idx.sort(1)  # [N,8732]\n",
    "\n",
    "        num_pos = pos.long().sum(1)  # [N,1]\n",
    "        num_neg = torch.clamp(3*num_pos, max=num_boxes-1)  # [N,1]\n",
    "\n",
    "        neg = rank < num_neg.unsqueeze(1).expand_as(rank) # ï¼ˆN,num_anc_8732)\n",
    "\n",
    "        return neg\n",
    "\n",
    "    def forward(self, loc_preds, loc_targets, conf_preds, conf_targets):\n",
    "        '''Compute loss between (loc_preds, loc_targets) and (conf_preds, conf_targets).\n",
    "        Args:\n",
    "          loc_preds: (tensor) predicted locations, sized [batch_size, 8732, 4].\n",
    "          loc_targets: (tensor) encoded target locations, sized [batch_size, 8732, 4].\n",
    "          conf_preds: (tensor) predicted class confidences, sized [batch_size, 8732, num_classes].\n",
    "          conf_targets: (tensor) encoded target classes, sized [batch_size, 8732].\n",
    "        loss:\n",
    "          (tensor) loss = SmoothL1Loss(loc_preds, loc_targets) + CrossEntropyLoss(conf_preds, conf_targets).\n",
    "        '''\n",
    "        batch_size, num_boxes, _ = loc_preds.size()\n",
    "        pos = conf_targets > 0  # [N,8732], pos means the box matched.\n",
    "        num_matched_boxes = pos.data.float().sum()\n",
    "        if num_matched_boxes == 0:\n",
    "            return torch.tensor([0], requires_grad=True)\n",
    "        \n",
    "        ################################################################\n",
    "        # loc_loss = SmoothL1Loss(pos_loc_preds, pos_loc_targets)\n",
    "        ################################################################\n",
    "        pos_mask = pos.unsqueeze(2).expand_as(loc_preds)    # [N,8732,4]\n",
    "        pos_loc_preds = loc_preds[pos_mask].view(-1,4)      # [#pos,4]\n",
    "        pos_loc_targets = loc_targets[pos_mask].view(-1,4)  # [#pos,4]\n",
    "        \n",
    "        loc_loss = F.smooth_l1_loss(pos_loc_preds, pos_loc_targets, size_average=False)\n",
    "\n",
    "        ################################################################\n",
    "        # conf_loss = CrossEntropyLoss(pos_conf_preds, pos_conf_targets)\n",
    "        #           + CrossEntropyLoss(neg_conf_preds, neg_conf_targets)\n",
    "        ################################################################\n",
    "        conf_loss = F.cross_entropy(conf_preds.view(-1,self.num_classes), \\\n",
    "                                            conf_targets.view(-1), reduce=False)  # [N*8732,]\n",
    "        neg = self.hard_negative_mining(conf_loss, pos)    # [N,8732]\n",
    "\n",
    "        pos_mask = pos.unsqueeze(2).expand_as(conf_preds)  # [N,8732,21]\n",
    "        neg_mask = neg.unsqueeze(2).expand_as(conf_preds)  # [N,8732,21]\n",
    "        mask = (pos_mask+neg_mask).gt(0)\n",
    "\n",
    "        pos_and_neg = (pos+neg).gt(0)\n",
    "        preds = conf_preds[mask].view(-1,self.num_classes)  # [#pos+#neg,21]\n",
    "        targets = conf_targets[pos_and_neg]                 # [#pos+#neg,]\n",
    "        conf_loss = F.cross_entropy(preds, targets, size_average=False)\n",
    "\n",
    "        loc_loss /= num_matched_boxes\n",
    "        conf_loss /= num_matched_boxes\n",
    "\n",
    "        return loc_loss + conf_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dir",
   "language": "python",
   "name": "env_dir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
