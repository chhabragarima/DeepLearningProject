{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "#from skimage import transform\n",
    "import sys\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import copy\n",
    "import torchvision.models as models\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "#from display import read_image, draw_boxes, draw_grid, draw_text\n",
    "#import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen GPU: 0\n"
     ]
    }
   ],
   "source": [
    "# with this function you set the value of the environment variable CUDA_VISIBLE_DEVICES\n",
    "# to set which GPU to use\n",
    "# it also reserves this amount of memory for your exclusive use. This might be important for \n",
    "# not having other people using the resources you need in shared systems\n",
    "# the homework was tested in a GPU with 4GB of memory, and running this function will require at least\n",
    "# as much\n",
    "# if you want to test in a GPU with less memory, you can call this function\n",
    "# with the argument minimum_memory_mb specifying how much memory from the GPU you want to reserve\n",
    "def define_gpu_to_use(minimum_memory_mb = 3800):\n",
    "    gpu_to_use = None\n",
    "    try: \n",
    "        os.environ['CUDA_VISIBLE_DEVICES']\n",
    "        print('GPU already assigned before: ' + str(os.environ['CUDA_VISIBLE_DEVICES']))\n",
    "        return\n",
    "    except:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    for i in range(16):\n",
    "        free_memory = !nvidia-smi --query-gpu=memory.free -i $i --format=csv,nounits,noheader\n",
    "        if free_memory[0] == 'No devices were found':\n",
    "            break\n",
    "        free_memory = int(free_memory[0])\n",
    "        if free_memory>minimum_memory_mb-500:\n",
    "            gpu_to_use = i\n",
    "            break\n",
    "    if gpu_to_use is None:\n",
    "        print('Could not find any GPU available with the required free memory of ' +str(minimum_memory_mb) + 'MB. Please use a different system for this assignment.')\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_to_use)\n",
    "        print('Chosen GPU: ' + str(gpu_to_use))\n",
    "        x = torch.rand((256,1024,minimum_memory_mb-500)).cuda()\n",
    "        x = torch.rand((1,1)).cuda()\n",
    "        del x\n",
    "define_gpu_to_use()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image, size):\n",
    "    img = cv2.imread(image, cv2.IMREAD_UNCHANGED)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (size, size))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, boxes_list):\n",
    "    for boxes in boxes_list:\n",
    "        cv2.rectangle(image, (int(boxes[0]-boxes[2]/2),int(boxes[1]-boxes[3]/2)),(int(boxes[0]+boxes[2]/2),int(boxes[1]+boxes[3]/2)),(0,0,255),2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_grid(img, pixel_step):\n",
    "    x = pixel_step\n",
    "    y = pixel_step\n",
    "\n",
    "    while x < img.shape[1]:\n",
    "        cv2.line(img, (x, 0), (x, img.shape[0]), color=(255, 255, 255))\n",
    "        x += pixel_step\n",
    "\n",
    "    while y < img.shape[0]:\n",
    "        cv2.line(img, (0, y), (img.shape[1], y), color=(255, 255, 255))\n",
    "        y += pixel_step\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text(img, text_list, location):\n",
    "    for text,loc in zip(text_list, location):\n",
    "        cv2.putText(img, text, (int(loc[0]), int(loc[1])), cv2.FONT_HERSHEY_COMPLEX,0.5, (255, 0, 0), 1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_normalize(image):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    img = normalize(torch.from_numpy(image))\n",
    "    return img.numpy()\n",
    "\n",
    "def preprocess(img, min_size=600, max_size=1000):\n",
    "    C,H,W = img.shape\n",
    "    scale1 = min_size/min(H,W)\n",
    "    scale2 = max_size/max(H,W)\n",
    "    scale = min(scale1, scale2)\n",
    "    img = img/255\n",
    "    img = transform.resize(img, (C,H*scale, W*scale), mode='reflect', anti_aliasing=False)\n",
    "    normalize = img_normalize\n",
    "    return normalize(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropBiggestCenteredInscribedSquare(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        longer_side = min(tensor.size)\n",
    "        horizontal_padding = (longer_side - tensor.size[0]) / 2\n",
    "        vertical_padding = (longer_side - tensor.size[1]) / 2\n",
    "        return tensor.crop(\n",
    "            (\n",
    "                -horizontal_padding,\n",
    "                -vertical_padding,\n",
    "                tensor.size[0] + horizontal_padding,\n",
    "                tensor.size[1] + vertical_padding\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(image_name_list, split_type):\n",
    "    np.random.seed(31)\n",
    "    np.random.shuffle(image_name_list)\n",
    "    np.random.seed()\n",
    "    if split_type == 'train':\n",
    "        image_name_list = image_name_list[:int(len(image_name_list)*0.6)]\n",
    "    elif split_type == 'validate':\n",
    "        image_name_list = image_name_list[int(len(image_name_list)*0.6):int(len(image_name_list)*0.8)]\n",
    "    elif split_type== 'test':\n",
    "        image_name_list = image_name_list[int(len(image_name_list)*0.8):]\n",
    "    return image_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class plant_leaf_disease_dataset(Dataset):\n",
    "#     def __init__(self, data_dir_bounding_box,data_dir_images,split=\"train\"):\n",
    "#         self.data_dir_bounding_box = data_dir_bounding_box\n",
    "#         self.data_dir_images = data_dir_images\n",
    "#         self.grid_size=20\n",
    "#         self.label_names = ['AppleScab','CedarAppleRust','CherryPowderyMildew','FrogEyeSpot','CherrySourPowderyMildew','CornMaizeCercosporaGrayLeafSpot','CornMaizeCommonRust','CornMaizeHealthy','CornMaizeNorthernLeafBlight','GrapeBlackRot','GrapeEscaBlackMeasles','GrapeLeafBlightIsariopsis','OrangeHaungLongbingCitrus','PeachBacterialSpot','PepperBellBacterialSpot','PotatoEarlyBlight','PotatoLateBlight','SquashPowderyMildew','StrawberryLeafScorch','TomatoBacterialSpot','TomatoEarlyBlight','TomatoLateBlight','TomatoLeafMold','TomatoSeptoriaLeafSpot','TomatoSpiderMitesTwoSpotted','TomatoTargetSpot','TomatoMosaicVirus','TomatoYellowLeafCurlVirus']\n",
    "#         columns=['image_name','label','bounding_boxes', 'new_bounding_boxes','xmin','ymin','xmax','ymax']\n",
    "#         combined_dataset = []\n",
    "#         for subdir, dirs, files in os.walk(self.data_dir_bounding_box):\n",
    "#             for file in files:\n",
    "#                 #print (file)\n",
    "#                 bounding_box, label_name, image_name=self.get_example(file) #ordering xmin, ymin, xmax, ymax\n",
    "#                 new_bounding_box = self.convert_to_center(bounding_box)\n",
    "#                 #print (bounding_box)\n",
    "#                 record_data = [image_name, label_name, bounding_box, new_bounding_box, bounding_box[:,0], bounding_box[:,1],bounding_box[:,2], bounding_box[:,3]]\n",
    "#                 combined_dataset.append(record_data)\n",
    "#         self.dataset_df = pd.DataFrame(combined_dataset, columns = columns)\n",
    "#         self.dataset_df['grid_boxes'] = self.dataset_df['bounding_boxes'].apply(lambda x:self.grid_formation(x))\n",
    "        \n",
    "#         image_name_list = pd.unique(self.dataset_df['image_name'])\n",
    "#         splitted_data = split_data(image_name_list,split)\n",
    "#         image_name_list = pd.DataFrame(splitted_data,columns=['image_name'])\n",
    "#         subset_data = pd.merge(image_name_list,self.dataset_df)\n",
    "#         #print (len(subset_data))\n",
    "#         self.dataset_df = subset_data\n",
    "#         self.images = self.dataset_df['image_name'].values\n",
    "#         self.targets = self.dataset_df['label'].values\n",
    "#         self.bounding_boxes = self.dataset_df['bounding_boxes'].values\n",
    "#         self.grid_boxes = self.dataset_df['grid_boxes'].values\n",
    "#         self.new_bounding_boxes = self.dataset_df['new_bounding_boxes'].values\n",
    "#         print (len(self.dataset_df))\n",
    "#         #print (self.dataset_df.head())    \n",
    "#         #print (self.grid_boxes[0].shape)\n",
    "    \n",
    "#     def grid_formation(self,b_boxes):\n",
    "#         img_original_size=256\n",
    "#         grid_mat = torch.zeros([1,1,img_original_size, img_original_size],dtype=torch.float)\n",
    "#         #print (b_boxes.shape)\n",
    "#         for b_box in b_boxes:\n",
    "#             #print (b_box[1,:].item())\n",
    "#             y1= int(b_box[1,:].item())\n",
    "#             y2=int(b_box[3,:].item())\n",
    "#             x1=int(b_box[0,:].item())\n",
    "#             x2=int(b_box[2,:].item())\n",
    "            \n",
    "#             grid_mat[:,:,y1:y2,x1:x2] = 1.0\n",
    "#         first_resize_size = (img_original_size//self.grid_size)*self.grid_size\n",
    "#         grid_mat = torch.nn.functional.interpolate(grid_mat, size = (first_resize_size, first_resize_size), mode = 'bilinear', align_corners = False)\n",
    "#         grid_mat = torch.nn.AvgPool2d(kernel_size = img_original_size//self.grid_size)(grid_mat)\n",
    "#         grid_mat = ((grid_mat[0,:,:,:][:]>0.5)*1.0).float()\n",
    "#         return grid_mat\n",
    "    \n",
    "#     def convert_to_center(self, b_boxes):\n",
    "#         new_box=[]\n",
    "#         for b_box in b_boxes:\n",
    "#             cx = b_box[0]+b_box[2]/2\n",
    "#             cy = b_box[1]+b_box[3]/2\n",
    "#             new_box.append([cx,cy,b_box[2], b_box[3]])\n",
    "#         return new_box\n",
    "        \n",
    "#     def get_example(self,file_name):\n",
    "#         #print ()\n",
    "#         annotate = ET.parse(os.path.join(self.data_dir_bounding_box,re.findall(\"[a-zA-Z]+\",file_name)[0],file_name))\n",
    "#         image_name = annotate.find('filename').text\n",
    "#         #bounding_box = list()\n",
    "#         #label=list()\n",
    "#         for obj in annotate.findall('object'):\n",
    "#             bounding_box_values = obj.find('bndbox')\n",
    "#             bounding_box = [int(bounding_box_values.find(tag).text) for tag in ('xmin','ymin','xmax','ymax')]\n",
    "#             label_name=obj.find('name').text.strip()\n",
    "#             bounding_box = np.vstack(bounding_box)\n",
    "#             bounding_box = torch.from_numpy(bounding_box[None,:,:]).float()\n",
    "#             #print (bounding_box)\n",
    "#             #label.append(label_name)\n",
    "#         #print (bounding_box)\n",
    "#         #print (label_name)\n",
    "#         #bounding_box = np.stack(bounding_box).astype(np.float32)\n",
    "#         #label = np.stack(label).astype(np.int32) #need to change this\n",
    "#         return bounding_box,label_name, image_name\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         set_of_transforms = transforms.Compose(\n",
    "#         [CropBiggestCenteredInscribedSquare(),\n",
    "#          transforms.Resize(256),\n",
    "#         transforms.ToTensor(), \n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                  std=[0.229, 0.224, 0.225])])\n",
    "#         image_val = set_of_transforms(Image.open(self.data_dir_images + '/' + self.targets[index]+'/'+ self.images[index]).convert('RGB'))\n",
    "#         return self.images[index],image_val, self.label_names.index(self.targets[index]),self.new_bounding_boxes[index] #, self.grid_boxes[index]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.images)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class plant_leaf_disease_dataset(Dataset):\n",
    "    def __init__(self, data_dir_bounding_box,data_dir_images,split=\"train\"):\n",
    "        self.data_dir_bounding_box = data_dir_bounding_box\n",
    "        self.data_dir_images = data_dir_images\n",
    "        self.grid_size=20\n",
    "        self.latest_bounding_boxes=[]\n",
    "        self.label_names = ['AppleScab','CedarAppleRust','CherryPowderyMildew','FrogEyeSpot','CherrySourPowderyMildew','CornMaizeCercosporaGrayLeafSpot','CornMaizeCommonRust','CornMaizeHealthy','CornMaizeNorthernLeafBlight','GrapeBlackRot','GrapeEscaBlackMeasles','GrapeLeafBlightIsariopsis','OrangeHaungLongbingCitrus','PeachBacterialSpot','PepperBellBacterialSpot','PotatoEarlyBlight','PotatoLateBlight','SquashPowderyMildew','StrawberryLeafScorch','TomatoBacterialSpot','TomatoEarlyBlight','TomatoLateBlight','TomatoLeafMold','TomatoSeptoriaLeafSpot','TomatoSpiderMitesTwoSpotted','TomatoTargetSpot','TomatoMosaicVirus','TomatoYellowLeafCurlVirus']\n",
    "        columns=['image_name','label','bounding_boxes', 'new_bounding_boxes','xmin','ymin','xmax','ymax']\n",
    "        combined_dataset = []\n",
    "        for subdir, dirs, files in os.walk(self.data_dir_bounding_box):\n",
    "            for file in files:\n",
    "                #print (file)\n",
    "                bounding_box, label_name, image_name=self.get_example(file) #ordering xmin, ymin, xmax, ymax\n",
    "                #print (bounding_box)\n",
    "                new_bounding_box = self.convert_to_center(bounding_box)\n",
    "                self.latest_bounding_boxes.append(new_bounding_box[None,:,:])\n",
    "                record_data = [image_name, label_name, bounding_box, new_bounding_box, bounding_box[:,0], bounding_box[:,1],bounding_box[:,2], bounding_box[:,3]]\n",
    "                combined_dataset.append(record_data)\n",
    "        self.dataset_df = pd.DataFrame(combined_dataset, columns = columns)\n",
    "        self.dataset_df['grid_boxes'] = self.dataset_df['bounding_boxes'].apply(lambda x:self.grid_formation(x))\n",
    "        \n",
    "        image_name_list = pd.unique(self.dataset_df['image_name'])\n",
    "        splitted_data = split_data(image_name_list,split)\n",
    "        image_name_list = pd.DataFrame(splitted_data,columns=['image_name'])\n",
    "        subset_data = pd.merge(image_name_list,self.dataset_df)\n",
    "        #print (len(subset_data))\n",
    "        self.dataset_df = subset_data\n",
    "        self.images = self.dataset_df['image_name'].values\n",
    "        self.targets = self.dataset_df['label'].values\n",
    "        self.bounding_boxes = self.dataset_df['bounding_boxes'].values\n",
    "        self.grid_boxes = self.dataset_df['grid_boxes'].values\n",
    "        self.new_bounding_boxes = self.dataset_df['new_bounding_boxes'].values\n",
    "        print (len(self.dataset_df))\n",
    "        #print (self.dataset_df.head())    \n",
    "        #print (self.grid_boxes[0].shape)\n",
    "    \n",
    "    def grid_formation(self,b_boxes):\n",
    "        img_original_size=256\n",
    "        grid_mat = torch.zeros([1,1,img_original_size, img_original_size],dtype=torch.float)\n",
    "        #print (b_boxes.shape)\n",
    "        for b_box in b_boxes:\n",
    "            #print (b_box)\n",
    "            #print (b_box[1,:].item())\n",
    "            y1= int(b_box[1])\n",
    "            y2=int(b_box[3])\n",
    "            x1=int(b_box[0])\n",
    "            x2=int(b_box[2])\n",
    "            \n",
    "            grid_mat[:,:,y1:y2,x1:x2] = 1.0\n",
    "        first_resize_size = (img_original_size//self.grid_size)*self.grid_size\n",
    "        grid_mat = torch.nn.functional.interpolate(grid_mat, size = (first_resize_size, first_resize_size), mode = 'bilinear', align_corners = False)\n",
    "        grid_mat = torch.nn.AvgPool2d(kernel_size = img_original_size//self.grid_size)(grid_mat)\n",
    "        grid_mat = ((grid_mat[0,:,:,:][:]>0.5)*1.0).float()\n",
    "        return grid_mat\n",
    "    \n",
    "    def convert_to_center(self, b_boxes):\n",
    "        new_box=[]\n",
    "        for b_box in b_boxes:\n",
    "            cx = b_box[0]+b_box[2]/2\n",
    "            cy = b_box[1]+b_box[3]/2\n",
    "            new_box.append([cx,cy,b_box[2], b_box[3]])\n",
    "        new_box = np.vstack(new_box)\n",
    "        new_box = torch.from_numpy(new_box).float()\n",
    "        return new_box\n",
    "        \n",
    "    def get_example(self,file_name):\n",
    "        #print ()\n",
    "        annotate = ET.parse(os.path.join(self.data_dir_bounding_box,re.findall(\"[a-zA-Z]+\",file_name)[0],file_name))\n",
    "        image_name = annotate.find('filename').text\n",
    "        bounding_box = list()\n",
    "        #label=list()\n",
    "        for obj in annotate.findall('object'):\n",
    "            bounding_box_values = obj.find('bndbox')\n",
    "            bounding_box.append([int(bounding_box_values.find(tag).text) for tag in ('xmin','ymin','xmax','ymax')])\n",
    "            label_name=obj.find('name').text.strip()\n",
    "            #bounding_box = np.stack(bounding_box)\n",
    "            #bounding_box = torch.from_numpy(bounding_box[None,:,:]).float()\n",
    "            #bounding_box = torch.from_numpy(np.array(bounding_box)[None,:,:]).float()\n",
    "            #bounding_box = torch.Tensor(bounding_box)\n",
    "            #print (bounding_box)\n",
    "            #label.append(self.label_names.index(label_name))\n",
    "            label = label_name\n",
    "        bounding_box = np.stack(bounding_box).astype(np.float32)\n",
    "        #label = np.stack(label).astype(np.int32)\n",
    "        #print (bounding_box)\n",
    "        #print (label_name)\n",
    "        #bounding_box = np.stack(bounding_box).astype(np.float32)\n",
    "        #label = np.stack(label).astype(np.int32) #need to change this\n",
    "        return bounding_box,label, image_name\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        set_of_transforms = transforms.Compose(\n",
    "        [CropBiggestCenteredInscribedSquare(),\n",
    "         transforms.Resize(256),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])\n",
    "        image_val = set_of_transforms(Image.open(self.data_dir_images + '/' + self.targets[index]+'/'+ self.images[index]).convert('RGB'))\n",
    "        return self.images[index],image_val, self.label_names.index(self.targets[index]),self.latest_bounding_boxes[index] #, self.grid_boxes[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n",
      "50\n",
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "path_to_bounding_box = \"../bounding_box/\"\n",
    "path_to_image_folder = \"../leafDataForBoudingBox/\"\n",
    "label_names_dataset = ['AppleScab','CedarAppleRust','CherryPowderyMildew','FrogEyeSpot','CherrySourPowderyMildew','CornMaizeCercosporaGrayLeafSpot','CornMaizeCommonRust','CornMaizeHealthy','CornMaizeNorthernLeafBlight','GrapeBlackRot','GrapeEscaBlackMeasles','GrapeLeafBlightIsariopsis','OrangeHaungLongbingCitrus','PeachBacterialSpot','PepperBellBacterialSpot','PotatoEarlyBlight','PotatoLateBlight','SquashPowderyMildew','StrawberryLeafScorch','TomatoBacterialSpot','TomatoEarlyBlight','TomatoLateBlight','TomatoLeafMold','TomatoSeptoriaLeafSpot','TomatoSpiderMitesTwoSpotted','TomatoTargetSpot','TomatoMosaicVirus','TomatoYellowLeafCurlVirus']\n",
    "train_dataset = plant_leaf_disease_dataset(path_to_bounding_box,path_to_image_folder)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle = True, batch_size = 28, num_workers = 3)\n",
    "validate_dataset = plant_leaf_disease_dataset(path_to_bounding_box,path_to_image_folder,split=\"validate\")\n",
    "test_dataset = plant_leaf_disease_dataset(path_to_bounding_box,path_to_image_folder,split=\"test\")\n",
    "#torch..utils.data.DataLoader(dataset,)\n",
    "validate_loader = torch.utils.data.DataLoader(validate_dataset, shuffle = True, batch_size = 28, num_workers = 3)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle = True, batch_size = 28, num_workers = 3)\n",
    "print (len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([[[180.5000, 182.5000, 253.0000, 199.0000]]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d125cc6f1db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#img = draw_boxes(img, i[3])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_names_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-dc1cb44a7476>\u001b[0m in \u001b[0;36mdraw_text\u001b[0;34m(img, text_list, location)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdraw_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFONT_HERSHEY_COMPLEX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "i = train_dataset.__getitem__(0)\n",
    "img=i[0]\n",
    "print (i[2])\n",
    "img = read_image(path_to_image_folder+\"/\"+label_names_dataset[i[2]]+\"/\"+img, 256)\n",
    "#img = draw_boxes(img, i[3])\n",
    "print (i[3])\n",
    "img = draw_text(img, label_names_dataset[i[2]], i[3])\n",
    "plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_grid = [4,2,1]\n",
    "anchor_zooms = [0.7,1.10,1.3]\n",
    "anchor_ratios = [(1.0,1.0),(1.0,0.5),(0.5,1.0)]\n",
    "anchor_scales = [(a*h,a*w) for a in anchor_zooms for (h,w) in anchor_ratios]\n",
    "anchor_offsets = [1/(2*o) for o in anchor_grid]\n",
    "num_boxes = len(anchor_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_x = [np.repeat(np.linspace(anchor_off,1-anchor_off, a_grid),a_grid) for (anchor_off,a_grid) in zip(anchor_offsets, anchor_grid)]\n",
    "anchor_x = np.concatenate(anchor_x)\n",
    "anchor_y = [np.tile(np.linspace(ao, 1-ao, ag), ag) for ao,ag in zip(anchor_offsets, anchor_grid)]\n",
    "anchor_y = np.concatenate(anchor_y)\n",
    "anchor_centers = np.repeat(np.stack([anchor_x, anchor_y], axis=1), num_boxes,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_sizes  = [np.array([[w/ag, h/ag] for _ in range(ag**2) for w, h in anchor_scales])\n",
    "                 for ag in anchor_grid]\n",
    "anchor_sizes = np.concatenate(anchor_sizes)\n",
    "\n",
    "anchors = np.concatenate([anchor_centers, anchor_sizes], axis=1)\n",
    "anchors = torch.from_numpy(anchors).float()\n",
    "anchors = anchors.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flatten_conv(x, num_boxes):\n",
    "    samples, channels, _, _ = x.size()\n",
    "    x = x.permute(0, 2, 3, 1).contiguous()\n",
    "    return x.view(samples, -1, int(channels/num_boxes))\n",
    "\n",
    "class StandardConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=2):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn(x)\n",
    "        return x        \n",
    "\n",
    "class OutputConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_boxes):\n",
    "        super().__init__()\n",
    "        self.num_boxes = num_boxes\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels, (len(train_dataset)+1)*num_boxes, kernel_size=3, padding=1)\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels, 4*num_boxes, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [class predictions, box coordinates]\n",
    "        return [flatten_conv(self.conv_1(x), self.num_boxes), \n",
    "                flatten_conv(self.conv_2(x), self.num_boxes)]\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, num_boxes):\n",
    "        super().__init__()\n",
    "\n",
    "        pretrained_model = list(models.vgg16(pretrained='imagenet').children())[:-1]\n",
    "        self.pretrained_model = torch.nn.Sequential(*pretrained_model)\n",
    "\n",
    "        self.std_conv_1 = StandardConv(512, 256, stride=1)\n",
    "        self.std_conv_2 = StandardConv(256, 256)\n",
    "        self.std_conv_3 = StandardConv(256, 256)\n",
    "        self.std_conv_4 = StandardConv(256, 256)\n",
    "\n",
    "        self.out_conv_1 = OutputConv(256, num_boxes)\n",
    "        self.out_conv_2 = OutputConv(256, num_boxes)\n",
    "        self.out_conv_3 = OutputConv(256, num_boxes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pretrained_model(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.std_conv_1(x)\n",
    "        x = self.std_conv_2(x)\n",
    "        output_class_1, output_bb_1 = self.out_conv_1(x)\n",
    "\n",
    "        x = self.std_conv_3(x)\n",
    "        output_class_2, output_bb_2 = self.out_conv_2(x)\n",
    "\n",
    "        x = self.std_conv_4(x)\n",
    "        output_class_3, output_bb_3 = self.out_conv_3(x)\n",
    "\n",
    "        # Class, bounding box\n",
    "        return [torch.cat([output_class_1, output_class_2, output_class_3], dim=1),\n",
    "                torch.cat([output_bb_1, output_bb_2, output_bb_3], dim=1)\n",
    "                ]\n",
    "\n",
    "    def change_freezing(self, mode=False):\n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_overlap(bb_true_i, anchors, jaccard_overlap):\n",
    "    jaccard_tensor = jaccard(anchors, bb_true_i)\n",
    "    _, max_overlap = torch.max(jaccard_tensor, dim=0)\n",
    "    \n",
    "    overlap_list = []    \n",
    "    for i in range(len(bb_true_i)):\n",
    "        threshold_overlap = (jaccard_tensor[:, i] > jaccard_overlap).nonzero()\n",
    "\n",
    "        if len(threshold_overlap) > 0:\n",
    "            threshold_overlap = threshold_overlap[:, 0]\n",
    "            overlap = torch.cat([max_overlap[i].view(1), threshold_overlap])\n",
    "            overlap = torch.unique(overlap)     \n",
    "        else:\n",
    "            overlap = max_overlap[i].view(1)\n",
    "        overlap_list.append(overlap)\n",
    "    return overlap_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2, device=\"cuda\", eps=1e-10):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        p = torch.sigmoid(input)\n",
    "        pt = p * target.float() + (1.0 - p) * (1 - target).float()\n",
    "        alpha_t = (1.0 - self.alpha) * target.float() + self.alpha * (1 - target).float()\n",
    "        loss = - 1.0 * torch.pow((1 - pt), self.gamma) * torch.log(pt + self.eps)\n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SSDLoss(torch.nn.Module):\n",
    "    def __init__(self, loc_factor, anchors, jaccard_overlap, **kwargs):\n",
    "        super().__init__()\n",
    "        self.fl = FocalLoss(**kwargs)\n",
    "        self.loc_factor = loc_factor\n",
    "        self.jaccard_overlap = jaccard_overlap\n",
    "        self.anchors = anchors\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot_encoding(labels, num_classes):\n",
    "        return torch.eye(num_classes)[labels]\n",
    "\n",
    "    @staticmethod\n",
    "    def loc_transformation(x, anchors, overlap_indicies):\n",
    "        # Doing location transformations according to SSD paper\n",
    "        return torch.cat([(x[:, 0:1] - anchors[overlap_indicies, 0:1]) / anchors[overlap_indicies, 2:3],\n",
    "                          (x[:, 1:2] - anchors[overlap_indicies, 1:2]) / anchors[overlap_indicies, 3:4],\n",
    "                          torch.log((x[:, 2:3] / anchors[overlap_indicies, 2:3])),\n",
    "                          torch.log((x[:, 3:4] / anchors[overlap_indicies, 3:4]))\n",
    "                         ], dim=1)\n",
    "\n",
    "    def forward(self, class_hat, bb_hat, class_true, bb_true):        \n",
    "        loc_loss = 0.0\n",
    "        class_loss = 0.0\n",
    "\n",
    "        for i in range(len(class_true)):  # Batch level\n",
    "            class_hat_i = class_hat[i, :, :]\n",
    "            bb_true_i = bb_true[i]\n",
    "            class_true_i = class_true[i]\n",
    "            class_target = torch.zeros(class_hat_i.shape[0]).long().cuda()\n",
    "\n",
    "            overlap_list = find_overlap(bb_true_i.squeeze(0), self.anchors, self.jaccard_overlap)\n",
    "\n",
    "            temp_loc_loss = 0.0\n",
    "            for j in range(len(overlap_list)):  # BB level\n",
    "                overlap = overlap_list[j]\n",
    "                class_target[overlap] = class_true_i[0, j]\n",
    "\n",
    "                input_ = bb_hat[i, overlap, :]\n",
    "                target_ = SSDLoss.loc_transformation(bb_true_i[0, j, :].expand((len(overlap), 4)), self.anchors, overlap)\n",
    "\n",
    "                temp_loc_loss += F.smooth_l1_loss(input=input_, target=target_, reduction=\"sum\") / len(overlap)\n",
    "            loc_loss += temp_loc_loss / class_true_i.shape[1]\n",
    "\n",
    "            class_target = SSDLoss.one_hot_encoding(class_target, len(id_cat) + 1).float().to(device)\n",
    "            class_loss += self.fl(class_hat_i, class_target) / class_true_i.shape[1]\n",
    "\n",
    "        loc_loss = loc_loss / len(class_true)\n",
    "        class_loss = class_loss / len(class_true)\n",
    "        loss = class_loss + loc_loss * self.loc_factor\n",
    "\n",
    "        return loss, loc_loss, class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = SSDLoss(loc_factor=5.0, anchors=anchors, jaccard_overlap=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "seed=42\n",
    "print (num_boxes)\n",
    "torch.manual_seed(seed)\n",
    "model = Model(num_boxes=num_boxes).cuda()\n",
    "model.change_freezing(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "lr = 1e-4\n",
    "wd = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(box_a: torch.Tensor, box_b: torch.Tensor) -> float:\n",
    "    intersection = intersect(box_a, box_b)\n",
    "    union = box_area(box_a).unsqueeze(1) + box_area(box_b).unsqueeze(0) - intersection\n",
    "    return intersection / union\n",
    "def intersect(box_a: torch.Tensor, box_b: torch.Tensor) -> float:\n",
    "    # Coverting (cx, cy, w, h) to (x1, y1, x2, y2) since its easier to extract min/max coordinates\n",
    "    temp_box_a, temp_box_b = center_2_hw(box_a), center_2_hw(box_b)\n",
    "    print (temp_box_a.shape)\n",
    "    print (temp_box_b.shape)\n",
    "    max_xy = torch.min(temp_box_a[:, None, 2:], temp_box_b[None, :, 2:])\n",
    "    min_xy = torch.max(temp_box_a[:, None, :2], temp_box_b[None, :, :2])\n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]\n",
    "def center_2_hw(box: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Converting (cx, cy, w, h) to (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    print (len(box))\n",
    "#     return torch.cat(\n",
    "#         [box[:, 0, None] - box[:, 2, None]/2,\n",
    "#          box[:, 1, None] - box[:, 3, None]/2,\n",
    "#          box[:, 0, None] + box[:, 2, None]/2,\n",
    "#          box[:, 1, None] + box[:, 3, None]/2\n",
    "#          ], dim=1)\n",
    "#     print (len(box[:,0]))\n",
    "#     print (\"done\")\n",
    "    return torch.cat(\n",
    "        [box[:, 0] - box[:, 2]/2,\n",
    "         box[:, 1] - box[:, 3]/2,\n",
    "         box[:, 0] + box[:, 2]/2,\n",
    "         box[:, 1] + box[:, 3]/2\n",
    "         ]) \n",
    "#     if len(box)>4 else \n",
    "#     torch.cat([box[0] - box[2]/2,\n",
    "#          box[1] - box[3]/2,\n",
    "#          box[0] + box[2]/2,\n",
    "#          box[1] + box[3]/2\n",
    "#          ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[102.5000,  79.5000, 117.0000,  69.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 96.5000, 139.5000, 117.0000, 139.0000]]],\n",
      "\n",
      "\n",
      "        [[[111.5000, 220.0000,  95.0000, 212.0000]]],\n",
      "\n",
      "\n",
      "        [[[326.5000, 102.0000, 241.0000, 110.0000]]],\n",
      "\n",
      "\n",
      "        [[[219.0000, 138.5000, 196.0000, 161.0000]]],\n",
      "\n",
      "\n",
      "        [[[165.0000, 161.5000, 194.0000, 179.0000]]],\n",
      "\n",
      "\n",
      "        [[[200.5000, 101.0000, 209.0000, 136.0000]]],\n",
      "\n",
      "\n",
      "        [[[175.0000, 134.0000, 174.0000, 146.0000]]],\n",
      "\n",
      "\n",
      "        [[[180.5000, 182.5000, 253.0000, 199.0000]]],\n",
      "\n",
      "\n",
      "        [[[199.5000, 164.0000, 179.0000, 186.0000]]],\n",
      "\n",
      "\n",
      "        [[[183.0000, 149.0000, 202.0000, 202.0000]]],\n",
      "\n",
      "\n",
      "        [[[139.0000,  58.5000, 138.0000,  77.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 59.5000, 164.0000,  65.0000, 140.0000]]],\n",
      "\n",
      "\n",
      "        [[[126.5000, 265.5000, 135.0000, 217.0000]]],\n",
      "\n",
      "\n",
      "        [[[188.5000, 148.5000, 167.0000, 187.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 98.0000, 124.0000, 138.0000, 154.0000]]],\n",
      "\n",
      "\n",
      "        [[[195.5000, 109.0000, 165.0000, 112.0000]]],\n",
      "\n",
      "\n",
      "        [[[230.0000, 150.0000, 204.0000, 202.0000]]],\n",
      "\n",
      "\n",
      "        [[[153.5000, 151.0000, 191.0000, 154.0000]]],\n",
      "\n",
      "\n",
      "        [[[184.5000, 184.0000, 203.0000, 198.0000]]],\n",
      "\n",
      "\n",
      "        [[[135.0000, 161.0000, 152.0000, 142.0000]]],\n",
      "\n",
      "\n",
      "        [[[231.0000, 181.0000, 202.0000, 146.0000]]],\n",
      "\n",
      "\n",
      "        [[[243.5000,  85.0000, 207.0000, 106.0000]]],\n",
      "\n",
      "\n",
      "        [[[170.5000,  57.0000, 175.0000,  64.0000]]],\n",
      "\n",
      "\n",
      "        [[[106.0000, 158.0000, 130.0000, 176.0000]]],\n",
      "\n",
      "\n",
      "        [[[151.5000, 141.0000, 185.0000, 168.0000]]],\n",
      "\n",
      "\n",
      "        [[[164.5000, 194.0000, 217.0000, 202.0000]]],\n",
      "\n",
      "\n",
      "        [[[176.0000, 186.5000, 202.0000, 201.0000]]]])\n",
      "189\n",
      "1\n",
      "torch.Size([756])\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2ea72935087c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclass_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#print (bb_hat.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/test_gpu/env_dir/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-e86d9cbdaa18>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, class_hat, bb_hat, class_true, bb_true)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mclass_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_hat_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0moverlap_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_overlap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb_true_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjaccard_overlap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtemp_loc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-29fab695cce6>\u001b[0m in \u001b[0;36mfind_overlap\u001b[0;34m(bb_true_i, anchors, jaccard_overlap)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_overlap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb_true_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaccard_overlap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mjaccard_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjaccard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_true_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_overlap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaccard_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moverlap_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-50b71c40aaac>\u001b[0m in \u001b[0;36mjaccard\u001b[0;34m(box_a, box_b)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mjaccard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_a\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_b\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mintersection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintersect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0munion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox_area\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbox_area\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mintersection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mintersection\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0munion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mintersect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_a\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_b\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-50b71c40aaac>\u001b[0m in \u001b[0;36mintersect\u001b[0;34m(box_a, box_b)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtemp_box_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtemp_box_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmax_xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_box_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_box_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmin_xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_box_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_box_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0minter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_xy\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_xy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "def ff(x1,x2,x3,x4):\n",
    "    return [x1,x2,x3,x4]\n",
    "torch.manual_seed(seed)\n",
    "for epoch in range(0, n_epochs+1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    loc_loss = 0.0\n",
    "    class_loss = 0.0\n",
    "    #self.images[index],image_val, self.label_names.index(self.targets[index]),self.new_bounding_boxes[index]\n",
    "    for (_,x, class_true,bb_true) in train_loader:\n",
    "        model.zero_grad()\n",
    "        print (bb_true)\n",
    "        x=x.cuda()\n",
    "        #class_true=class_true.cuda()\n",
    "        #print (bb_true)\n",
    "        #bb_true = list(map(ff, bb_true[0].numpy(),bb_true[1].numpy(), bb_true[2].numpy(), bb_true[3].numpy()))\n",
    "        #bb_true = bb_true.cuda()\n",
    "        #print (bb_true)\n",
    "        #bb_true=torch.FloatTensor(bb_true).cuda()\n",
    "        class_hat, bb_hat = model(x)\n",
    "        #print (bb_hat.shape)\n",
    "        batch_loss, batch_loc, batch_class = loss(class_hat, bb_hat, class_true, bb_true)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        class_loss += batch_class\n",
    "        loc_loss += batch_loc\n",
    "        train_loss += batch_loss\n",
    "\n",
    "    train_loss = (train_loss/len(train_loader)).detach().cpu().numpy()\n",
    "    loc_loss = (loc_loss/len(train_loader)).detach().cpu().numpy()\n",
    "    class_loss = (class_loss/len(train_loader)).detach().cpu().numpy()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"----- epoch {epoch} -----\")\n",
    "        print(\"Train loss: {:.4f}\".format(train_loss))\n",
    "        print(\"Loc loss: {:.4f}\".format(loc_loss))\n",
    "        print(\"Class loss: {:.4f}\".format(class_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dir",
   "language": "python",
   "name": "env_dir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
